{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCC CBB\n",
    "\n",
    "Desenvolvido por Ricardo e Thyago.\n",
    "\n",
    "#### Objetivo: \n",
    "\n",
    "Aplicação das técnicas de classificação SVM, MLP e Tensorflow junto ao algoritmo bio-inspirado PSO e aplicado na base de dados Telco Customer Churn.\n",
    "\n",
    "Neste script é aplicado os algoritmos de classificação SVM, MLP e Tensorflow e o modelo estatístico\n",
    "com seu ponderamento feito pelos algoritmos Bio-inspirados.\n",
    "\n",
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICAÇÃO COM A BASE DE DADOS CUSTUMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import bigml.api\n",
    "import os\n",
    "import pandas as pd\n",
    "from bigml.api import BigML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicaSVM(X_train, X_test, y_train): \n",
    "    \n",
    "    model_svm = SVC(probability = True, kernel = 'rbf', gamma = 'scale')\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    predictionsSVM = model_svm.predict_proba(X_test)\n",
    "    \n",
    "    return predictionsSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import do KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "#tensorflow.keras.models\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicaTensor(X_train, X_test, y_train, y_test, dim_input):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, input_dim=dim_input, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=100)\n",
    "    \n",
    "    predictionsTensor = model.predict_proba(X_test)\n",
    "\n",
    "    return predictionsTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\Users\\\\Thyago M\\\\Desktop\\\\ffmpeg.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import do MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, model_selection, metrics\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicaMLP(X_train, X_test, y_train):\n",
    "    #X_train, X_test, y_train, y_test = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "   \n",
    "    modelMLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    modelMLP.fit(X_train, y_train)\n",
    "    predictionsMLP = modelMLP.predict_proba(X_test)\n",
    "    \n",
    "    return predictionsMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converte_binario(palavra):\n",
    "    if palavra=='Yes' or palavra==True:\n",
    "        return 0\n",
    "    elif palavra=='No' or palavra==False:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WA_Fn-UseC_-Telco-Customer-Churn\n",
    "def descarregaBaseDadosCustomer():\n",
    "\n",
    "    df_WA = pd.read_csv(os.getcwd()+'\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "    df_WA_target = df_WA['Churn'].apply(converte_binario) \n",
    "    df_WA['PhoneService'] = df_WA['PhoneService'].apply(converte_binario)\n",
    "    df_WA.drop('Churn', axis=1, inplace=True)\n",
    "    df_WA.drop('customerID', axis=1, inplace=True)\n",
    "    df_WA.drop('gender', axis=1, inplace=True)\n",
    "    df_WA.drop('Partner', axis=1, inplace=True)\n",
    "    df_WA.drop('Dependents', axis=1, inplace=True)\n",
    "    df_WA.drop('tenure', axis=1, inplace=True)\n",
    "    df_WA.drop('PaperlessBilling', axis=1, inplace=True)\n",
    "    df_WA.drop('PaymentMethod', axis=1, inplace=True)\n",
    "    df_WA.drop('Contract', axis=1, inplace=True)\n",
    "    \n",
    "    TotalCharges = []\n",
    "    for x in df_WA['TotalCharges']:\n",
    "        x = str(x)\n",
    "        if('.' in x or ',' in x):\n",
    "            x = x.replace(',','.')\n",
    "            TotalCharges.append(x)\n",
    "\n",
    "    #print(nova_lista1)\n",
    "    x = 0\n",
    "    MonthlyCharges = []\n",
    "    for x in df_WA['MonthlyCharges']:\n",
    "        x = str(x)\n",
    "        if('.' in x or ',' in x):\n",
    "            x = x.replace(',','.')\n",
    "            MonthlyCharges.append(x)\n",
    "\n",
    "    df_WA.drop(['TotalCharges', 'MonthlyCharges'], axis=1)\n",
    "\n",
    "    TotalCharges_ = pd.Series(TotalCharges)\n",
    "    MonthlyCharges_ = pd.Series(MonthlyCharges)\n",
    "\n",
    "    df_WA['TotalCharges'] = TotalCharges_\n",
    "    df_WA['MonthlyCharges'] = MonthlyCharges_\n",
    "    \n",
    "    #AGRUPAMENTO DE DADOS COM MAIS DE TRÊS TIPOS DE PARAMETROS\n",
    "    \n",
    "    internetServices = df_WA['InternetService'].value_counts()\n",
    "    MultipleLines = df_WA['MultipleLines'].value_counts()   \n",
    "    OnlineSecurity = df_WA['OnlineSecurity'].value_counts()\n",
    "    OnlineBackup = df_WA['OnlineBackup'].value_counts()\n",
    "    DeviceProtection = df_WA['DeviceProtection'].value_counts()\n",
    "    TechSupport = df_WA['TechSupport'].value_counts()\n",
    "    StreamingTV = df_WA['StreamingTV'].value_counts()\n",
    "    StreamingMovies = df_WA['StreamingMovies'].value_counts()\n",
    "    \n",
    "    array_internetServices = []\n",
    "    array_MultipleLines = []\n",
    "    array_OnlineSecurity = []\n",
    "    array_OnlineBackup =[]\n",
    "    array_DeviceProtection = []\n",
    "    array_TechSupport =[]\n",
    "    array_StreamingTV = []\n",
    "    array_StreamingMovies = []\n",
    "    \n",
    "    i = 0\n",
    "    #---CAMPO INTERNETSERVICES------\n",
    "    for index, val in internetServices.iteritems():\n",
    "        array_internetServices.append(index)    \n",
    "    for servico in array_internetServices:\n",
    "        df_WA['InternetService'] = df_WA['InternetService'].replace(to_replace=servico, value=i)\n",
    "        i = i+1\n",
    "        \n",
    "    #---CAMPO MULTIPLES LINES------\n",
    "    \n",
    "    for index, val in MultipleLines.iteritems():\n",
    "        array_MultipleLines.append(index)\n",
    "        i=0\n",
    "    for multiple in array_MultipleLines:\n",
    "        df_WA['MultipleLines'] = df_WA['MultipleLines'].replace(to_replace=multiple, value=i)\n",
    "        i= i+1\n",
    "     #-----campo OnlineSecurity\n",
    "    for index, val in OnlineSecurity.iteritems():\n",
    "        array_OnlineSecurity.append(index)\n",
    "        i=0\n",
    "    for online in array_OnlineSecurity:\n",
    "        df_WA['OnlineSecurity'] = df_WA['OnlineSecurity'].replace(to_replace=online, value=i)\n",
    "        i= i+1\n",
    "        \n",
    "    #--campo OnlineBackup\n",
    "    for index, val in OnlineBackup.iteritems():\n",
    "        array_OnlineBackup.append(index)\n",
    "        i=0\n",
    "    for backup in array_OnlineBackup:\n",
    "        df_WA['OnlineBackup'] = df_WA['OnlineBackup'].replace(to_replace=backup, value=i)\n",
    "        i= i+1\n",
    "    #----campo DeviceProtection\n",
    "    for index, val in DeviceProtection.iteritems():\n",
    "        array_DeviceProtection.append(index)\n",
    "        i=0\n",
    "    for device in array_DeviceProtection:\n",
    "        df_WA['DeviceProtection'] = df_WA['DeviceProtection'].replace(to_replace=device, value=i)\n",
    "        i= i+1\n",
    "    #---campo TechSupport\n",
    "    for index, val in TechSupport.iteritems():\n",
    "        array_TechSupport.append(index)\n",
    "        i=0\n",
    "    for support in array_TechSupport:\n",
    "        df_WA['TechSupport'] = df_WA['TechSupport'].replace(to_replace=support, value=i)\n",
    "        i= i+1\n",
    "    #---StreamingTV\n",
    "    for index, val in StreamingTV.iteritems():\n",
    "        array_StreamingTV.append(index)\n",
    "        i=0\n",
    "    for streamingtv in array_StreamingTV:\n",
    "        df_WA['StreamingTV'] = df_WA['StreamingTV'].replace(to_replace=streamingtv, value=i)\n",
    "        i= i+1\n",
    "    #---StreamingMovies\n",
    "    for index, val in StreamingMovies.iteritems():\n",
    "        array_StreamingMovies.append(index)\n",
    "        i=0\n",
    "    for streamingmovies in array_StreamingMovies:\n",
    "        df_WA['StreamingMovies'] = df_WA['StreamingMovies'].replace(to_replace=streamingmovies, value=i)\n",
    "        i= i+1\n",
    "   \n",
    "    \n",
    "    df_WA.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    print(df_WA_target.value_counts())\n",
    "\n",
    "\n",
    "    return df_WA, df_WA_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APLICA K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicaKFold(x_pca, df_weather_target):\n",
    "    divisao = 0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_pca, df_weather_target, test_size=divisao)\n",
    "   \n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_acertos(predicoes, y_test):\n",
    "    qtd_acertos = []\n",
    "    for i in range(predicoes.shape[0]):\n",
    "        if predicoes[i] == y_test[i]:\n",
    "            qtd_acertos.append(1)\n",
    "        else:\n",
    "            qtd_acertos.append(0)\n",
    "    return np.asarray(qtd_acertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicaKFoldVariado(x_pca, df_bigml_target,divisao):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_pca, df_bigml_target, test_size=divisao)\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio Inspirados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sw(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.__Positions = []\n",
    "        self.__Gbest = []\n",
    "\n",
    "    def _set_Gbest(self, Gbest):\n",
    "        self.__Gbest = Gbest\n",
    "\n",
    "    def _points(self, agents):\n",
    "        self.__Positions.append([list(i) for i in agents])\n",
    "\n",
    "    def get_agents(self):\n",
    "        \"\"\"Returns a history of all agents of the algorithm (return type:\n",
    "        list)\"\"\"\n",
    "\n",
    "        return self.__Positions\n",
    "\n",
    "    def get_Gbest(self):\n",
    "        \"\"\"Return the best position of algorithm (return type: list)\"\"\"\n",
    "\n",
    "        return list(self.__Gbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "\n",
    "class fa(sw):\n",
    "    \"\"\"\n",
    "    Firefly Algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, n, function, lb, ub, dimension, iteration, y_test, vetor_pbest, vetor_acertos, csi=1, psi=1,\n",
    "                 alpha0=10, alpha1=0.5, norm0=0, norm1=0.1):\n",
    "        \"\"\"\n",
    "        :param n: number of agents\n",
    "        :param function: test function\n",
    "        :param lb: lower limits for plot axes\n",
    "        :param ub: upper limits for plot axes\n",
    "        :param dimension: space dimension\n",
    "        :param iteration: number of iterations\n",
    "        :param csi: mutual attraction (default value is 1)\n",
    "        :param psi: light absorption coefficient of the medium\n",
    "        (default value is 1)\n",
    "        :param alpha0: initial value of the free randomization parameter alpha\n",
    "        (default value is 1)\n",
    "        :param alpha1: final value of the free randomization parameter alpha\n",
    "        (default value is 0.1)\n",
    "        :param norm0: first parameter for a normal (Gaussian) distribution\n",
    "        (default value is 0)\n",
    "        :param norm1: second parameter for a normal (Gaussian) distribution\n",
    "        (default value is 0.1)\n",
    "        \"\"\"\n",
    "\n",
    "        super(fa, self).__init__()\n",
    "\n",
    "        self.__agents = np.random.uniform(lb, ub, (n, dimension))\n",
    "        self._points(self.__agents)\n",
    "        Pbest = self.__agents[np.array([function(x, y_test)\n",
    "                                        for x in self.__agents]).argmax()]\n",
    "        Gbest = Pbest\n",
    "        vetor_pbest.append(Pbest)\n",
    "        vetor_acertos.append(function(Pbest, y_test))\n",
    "        for t in range(iteration):\n",
    "            alpha = alpha1 + (alpha0 - alpha1) * exp(-t)\n",
    "            for i in range(n):\n",
    "                fitness = [function(x, y_test) for x in self.__agents]\n",
    "                for j in range(n):\n",
    "                    if fitness[i] < fitness[j]:\n",
    "                        self.__move(i, j, t, csi, psi, alpha, dimension,\n",
    "                                    norm0, norm1)\n",
    "                    else:\n",
    "                        self.__agents[i] += np.random.normal(norm0, norm1,\n",
    "                                                             dimension)\n",
    "\n",
    "            self.__agents = np.clip(self.__agents, lb, ub)\n",
    "            self._points(self.__agents)\n",
    "            \n",
    "            Pbest = self.__agents[\n",
    "                np.array([function(x, y_test) for x in self.__agents]).argmax()]\n",
    "            if function(Pbest, y_test) > function(Gbest, y_test):\n",
    "                Gbest = Pbest\n",
    "            vetor_pbest.append(Gbest)\n",
    "            vetor_acertos.append(function(Gbest, y_test))\n",
    "        self._set_Gbest(Gbest)\n",
    "\n",
    "    def __move(self, i, j, t, csi, psi, alpha, dimension, norm0, norm1):\n",
    "        r = np.linalg.norm(self.__agents[i] - self.__agents[j])\n",
    "        beta = csi / (1 + psi * r ** 2)\n",
    "        self.__agents[i] = self.__agents[j] + beta * (\n",
    "            self.__agents[i] - self.__agents[j]) + alpha * exp(-t) * \\\n",
    "                                                   np.random.normal(norm0,\n",
    "                                                                    norm1,\n",
    "                                                                    dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swa(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.__Positions = []\n",
    "        self.__Gbest = []\n",
    "\n",
    "    def _set_Gbest(self, __Gbest):\n",
    "        self.__Gbest = __Gbest\n",
    "\n",
    "    def _points(self, agents):\n",
    "        self.__Positions.append([list(i) for i in agents])\n",
    "\n",
    "    def get_agents(self):\n",
    "        \"\"\"Returns a history of all agents of the algorithm (return type:\n",
    "        list)\"\"\"\n",
    "\n",
    "        return self.__Positions\n",
    "\n",
    "    def get_Gbest(self):\n",
    "        \"\"\"Return the best position of algorithm (return type: list)\"\"\"\n",
    "\n",
    "        return list(self.__Gbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class pso(swa):\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, function, lb, ub, dimension, iteration, y_test_svm, vetor_pbest, vetor_acertos, w=0.5, c1=1,\n",
    "                 c2=1):\n",
    "        \"\"\"\n",
    "        :param n: number of agents\n",
    "        :param function: test function\n",
    "        :param lb: lower limits for plot axes\n",
    "        :param ub: upper limits for plot azes\n",
    "        :param dimension: space dimension\n",
    "        :param iteration: the number of iterations\n",
    "        :param w: balance between the range of research and consideration for\n",
    "        suboptimal decisions found (default value is 0.5):\n",
    "        w>1 the particle velocity increases, they fly apart and inspect\n",
    "         the space more carefully;\n",
    "        w<1 particle velocity decreases, convergence speed depends\n",
    "        on parameters c1 and c2 ;\n",
    "        :param c1: ratio between \"cognitive\" and \"social\" component\n",
    "        (default value is 1)\n",
    "        :param c2: ratio between \"cognitive\" and \"social\" component\n",
    "        (default value is 1)\n",
    "        \"\"\"\n",
    "\n",
    "        super(pso, self).__init__()\n",
    "\n",
    "        self.__agents = np.random.uniform(lb, ub, (n, dimension))\n",
    "        velocity = np.zeros((n, dimension))\n",
    "        self._points(self.__agents)\n",
    "\n",
    "        Pbest = self.__agents[np.array([function(x, y_test_svm)\n",
    "                                        for x in self.__agents]).argmax()]\n",
    "        Gbest = Pbest\n",
    "        vetor_pbest.append(Pbest)\n",
    "        vetor_acertos.append(function(Pbest, y_test_svm))\n",
    "        \n",
    "        for t in range(iteration):\n",
    "            r1 = np.random.random((n, dimension))\n",
    "            r2 = np.random.random((n, dimension))\n",
    "            velocity = w * velocity + c1 * r1 * (\n",
    "                Pbest - self.__agents) + c2 * r2 * (\n",
    "                Gbest - self.__agents)\n",
    "            self.__agents += velocity\n",
    "            self.__agents = np.clip(self.__agents, lb, ub)\n",
    "            self._points(self.__agents)\n",
    "         \n",
    "            Pbest = self.__agents[\n",
    "                np.array([function(x, y_test_svm) for x in self.__agents]).argmax()]\n",
    "            if function(Pbest, y_test_svm) > function(Gbest, y_test_svm):\n",
    "                Gbest = Pbest\n",
    "            vetor_pbest.append(Pbest)\n",
    "            vetor_acertos.append(function(Pbest, y_test_svm))\n",
    "        \n",
    "        self._set_Gbest(Gbest)\n",
    "        print(Gbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicações FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\Thyago M\\\\Desktop\\\\APLICA\\xc3\\x87\\xc3\\x95ES TCC - RECENTE\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv' does not exist: b'C:\\\\Users\\\\Thyago M\\\\Desktop\\\\APLICA\\xc3\\x87\\xc3\\x95ES TCC - RECENTE\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-00af8e143929>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#from SwarmPackagePy import testFunctions as tf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_bigml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_bigml_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescarregaBaseDadosCustomer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0bad9ff5781f>\u001b[0m in \u001b[0;36mdescarregaBaseDadosCustomer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdescarregaBaseDadosCustomer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf_WA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf_WA_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_WA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Churn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverte_binario\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda 3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda 3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda 3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda 3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda 3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\Thyago M\\\\Desktop\\\\APLICA\\xc3\\x87\\xc3\\x95ES TCC - RECENTE\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv' does not exist: b'C:\\\\Users\\\\Thyago M\\\\Desktop\\\\APLICA\\xc3\\x87\\xc3\\x95ES TCC - RECENTE\\\\WA_Fn-UseC_-Telco-Customer-Churn.csv'"
     ]
    }
   ],
   "source": [
    "#import SwarmPackagePy\n",
    "import matplotlib.pyplot as plt\n",
    "#from SwarmPackagePy import testFunctions as tf\n",
    "\n",
    "df_bigml, df_bigml_target = descarregaBaseDadosCustomer()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "random = np.random.randint(1,1000)\n",
    "scaler.fit(df_bigml)\n",
    "\n",
    "scaled_data = scaler.transform(df_bigml)  \n",
    "\n",
    "pca_svm = PCA(n_components = 11)\n",
    "pca_svm.fit(scaled_data)\n",
    "x_pca_svm = pca_svm.transform(scaled_data)\n",
    "\n",
    "pca_tensorflow = PCA(n_components=9)\n",
    "pca_tensorflow.fit(scaled_data)\n",
    "x_pca_tensorflow = pca_tensorflow.transform(scaled_data)\n",
    "#predictionsTensorflow = aplicaTensor(X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, 17)\n",
    "\n",
    "pca_MLP = PCA(n_components = 11)\n",
    "pca_MLP.fit(scaled_data)\n",
    "x_pca_mlp = pca_MLP.transform(scaled_data)\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = aplicaKFold2(x_pca_svm, df_bigml_target, 0.1, random)\n",
    "X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, y_test_tensorflow = aplicaKFold2(x_pca_tensorflow, df_bigml_target, 0.1, random)\n",
    "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = aplicaKFold2(x_pca_mlp, df_bigml_target, 0.1, random)\n",
    "\n",
    "predictionsSVM = aplicaSVM(X_train_svm, X_test_svm, y_train_svm)\n",
    "predictionsTensorflow = aplicaTensor(X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, y_test_tensorflow, 9)\n",
    "predictionsMLP = aplicaMLP(X_train_mlp, X_test_mlp, y_train_mlp)\n",
    "\n",
    "predictionsTensorflowAjustado = []\n",
    "\n",
    "for row in predictionsTensorflow:\n",
    "    predictionsTensorflowAjustado.append(row[0])\n",
    "predictionsTensorflowAjustado = np.asarray(predictionsTensorflowAjustado)\n",
    "\n",
    "vetor_pbest = []\n",
    "vetor_acertos = []\n",
    "\n",
    "alh = fa(40, retorna_acertos, 0, 1, 2, 20, y_test_svm)\n",
    "#alh1 = pso(40, retorna_acertos, 0, 1, 3, 20,  y_test_tensorflow, vetor_pbest,vetor_acertos )\n",
    "arr1 = np.array(alh.get_agents())\n",
    "animationFA(alh.get_agents(), retorna_acertos, 0, 1, y_test_svm, sr=True)\n",
    "animation3D(alh.get_agents(), retorna_acertos, 0, 1, y_test_svm, sr=True)\n",
    "print(alh.get_Gbest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APlicação do Modelo estatístico para o FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_alpha = ( ( 1 - predictionsSVM[:,1]) ** alh.get_Gbest()[0] )\n",
    "tensorflow_beta =  ( ( 1 - predictionsTensorflowAjustado) ** alh.get_Gbest()[1])\n",
    "mlp_gamma = ( ( 1 - predictionsMLP[:,1] ) ** alh.get_Gbest()[2])\n",
    "predicoes = 1 - (svm_alpha*tensorflow_beta*mlp_gamma)\n",
    "qtd_acertos = np.subtract(y_test_svm,np.round(predicoes))\n",
    "np.count_nonzero(qtd_acertos == 0)/len(y_test_svm) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APlicação PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SwarmPackagePy\n",
    "import matplotlib.pyplot as plt\n",
    "#from SwarmPackagePy import testFunctions as tf\n",
    "\n",
    "df_bigml, df_bigml_target = descarregaBaseDadosCustomer()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "random = np.random.randint(1,1000)\n",
    "scaler.fit(df_bigml)\n",
    "\n",
    "scaled_data = scaler.transform(df_bigml)  \n",
    "\n",
    "pca_svm = PCA(n_components = 11)\n",
    "pca_svm.fit(scaled_data)\n",
    "x_pca_svm = pca_svm.transform(scaled_data)\n",
    "\n",
    "pca_tensorflow = PCA(n_components=9)\n",
    "pca_tensorflow.fit(scaled_data)\n",
    "x_pca_tensorflow = pca_tensorflow.transform(scaled_data)\n",
    "#predictionsTensorflow = aplicaTensor(X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, 17)\n",
    "\n",
    "pca_MLP = PCA(n_components = 11)\n",
    "pca_MLP.fit(scaled_data)\n",
    "x_pca_mlp = pca_MLP.transform(scaled_data)\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = aplicaKFold2(x_pca_svm, df_bigml_target, 0.1, random)\n",
    "X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, y_test_tensorflow = aplicaKFold2(x_pca_tensorflow, df_bigml_target, 0.1, random)\n",
    "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = aplicaKFold2(x_pca_mlp, df_bigml_target, 0.1, random)\n",
    "\n",
    "predictionsSVM = aplicaSVM(X_train_svm, X_test_svm, y_train_svm)\n",
    "predictionsTensorflow = aplicaTensor(X_train_tensorflow, X_test_tensorflow, y_train_tensorflow, y_test_tensorflow, 9)\n",
    "predictionsMLP = aplicaMLP(X_train_mlp, X_test_mlp, y_train_mlp)\n",
    "\n",
    "predictionsTensorflowAjustado = []\n",
    "\n",
    "for row in predictionsTensorflow:\n",
    "    predictionsTensorflowAjustado.append(row[0])\n",
    "predictionsTensorflowAjustado = np.asarray(predictionsTensorflowAjustado)\n",
    "\n",
    "vetor_pbest = []\n",
    "vetor_acertos = []\n",
    "\n",
    "#alh = fa(40, retorna_acertos, 0, 1, 2, 20, y_test_svm)\n",
    "alh1 = pso(40, retorna_acertos, 0, 1, 3, 20,  y_test_tensorflow, vetor_pbest,vetor_acertos )\n",
    "arr1 = np.array(alh1.get_agents())\n",
    "animationFA(alh1.get_agents(), retorna_acertos, 0, 1, y_test_svm, sr=True)\n",
    "animation3D(alh1.get_agents(), retorna_acertos, 0, 1, y_test_svm, sr=True)\n",
    "print(alh1.get_Gbest())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do Modelo estatístico para o PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_alpha = ( ( 1 - predictionsSVM[:,1]) ** alh1.get_Gbest()[0] )\n",
    "tensorflow_beta =  ( ( 1 - predictionsTensorflowAjustado) ** alh1.get_Gbest()[1])\n",
    "mlp_gamma = ( ( 1 - predictionsMLP[:,1] ) ** alh1.get_Gbest()[2])\n",
    "predicoes = 1 - (svm_alpha*tensorflow_beta*mlp_gamma)\n",
    "qtd_acertos = np.subtract(y_test_svm,np.round(predicoes))\n",
    "np.count_nonzero(qtd_acertos == 0)/len(y_test_svm) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
